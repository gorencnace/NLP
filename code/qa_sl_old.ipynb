{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer, BertForMaskedLM, AdamW\n",
    "from tqdm.auto import tqdm\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['..\\\\data\\\\oscar_sl\\\\text_0.txt',\n",
       " '..\\\\data\\\\oscar_sl\\\\text_1.txt',\n",
       " '..\\\\data\\\\oscar_sl\\\\text_10.txt',\n",
       " '..\\\\data\\\\oscar_sl\\\\text_100.txt',\n",
       " '..\\\\data\\\\oscar_sl\\\\text_101.txt']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path('../data/oscar_sl').glob('**/*.txt')]\n",
    "print(len(paths))\n",
    "paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [CLS]\n",
      "4084 tukaj\n",
      "2039 lahko\n",
      "5407 uporabnik\n",
      "4 [MASK]\n",
      "54542 napise\n",
      "63277 poljuben\n",
      "18591 stavek\n",
      "90 v\n",
      "0 [PAD]\n",
      "5961 sloven\n",
      "14307 ##sci\n",
      "1935 ##ni\n",
      "18 .\n",
      "3 [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python3-8-10\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1653: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('../data/bert_sl/sl-vocab.txt')\n",
    "with open('../data/bert_sl/sl-vocab.txt', 'r', encoding='utf-8') as fp:\n",
    "    vocab = fp.read().split('\\n')\n",
    "\n",
    "stavek = 'Tukaj lahko uporabnik [MASK] napiše poljuben stavek v [PAD] slovenščini.'\n",
    "tokens = tokenizer(stavek)['input_ids']\n",
    "for t in tokens:\n",
    "    print(str(t) + \" \" + vocab[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "alphabets= \"([A-Za-z])\"\n",
    "lowercase = \"[.][ ]([a-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \").replace(\"\\\\s\",\" \").replace(\"\\s\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    text = re.sub(lowercase,\"<prd> \\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlm_data = []\n",
    "for p in paths[:10]:\n",
    "    with open(p, 'r',encoding='utf-8') as f:\n",
    "        for lines in f.readlines():\n",
    "            lines = split_into_sentences(lines)\n",
    "            for line in lines:\n",
    "                if len(line.split(\" \")) < max_length - 30:\n",
    "                    if len(tokenizer(line)['input_ids']) <= max_length:\n",
    "                        mlm_data.append(line)\n",
    "mlm_data[10:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "string = \"\"\n",
    "for tmp in mlm_data:\n",
    "    l = len(tokenizer(tmp)['input_ids'])\n",
    "    if l > max_len:\n",
    "        max_len = l\n",
    "        string = tmp\n",
    "print(max_len)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(mlm_data[:50000], return_tensors='pt', max_length=max_length, truncation=True, padding='max_length')\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 2) * (inputs.input_ids != 4) * (inputs.input_ids != 0) # we don't want to mask [CLS], [MASK] and [PAD] tokens\n",
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(torch.flatten(mask_arr[i].nonzero()).tolist())\n",
    "    inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "dataset = Dataset(inputs)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "optim = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2 # if number is large it can overtrain easily\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        optim.zero_grad()\n",
    "        input_ids = torch.tensor(batch['input_ids'], device=device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask'], device=device)\n",
    "        labels = torch.tensor(batch['labels'], device=device)\n",
    "        #print(input_ids.size())\n",
    "        #print(attention_mask.size())\n",
    "        #print(labels.size())\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir('../data/bert_mlm')\n",
    "\n",
    "torch.save(model.state_dict(), '../data/bert_mlm/weights_pretrain')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'title': 'Normani', 'paragraphs': [{'qas': [{...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'title': 'Computational_complexity_theory', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'title': 'Southern_California', 'paragraphs':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'title': 'Sky_(Združeno kraljestvo)', 'paragr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'title': 'Victoria_(Avstralija)', 'paragraphs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data\n",
       "0  {'title': 'Normani', 'paragraphs': [{'qas': [{...\n",
       "1  {'title': 'Computational_complexity_theory', '...\n",
       "2  {'title': 'Southern_California', 'paragraphs':...\n",
       "3  {'title': 'Sky_(Združeno kraljestvo)', 'paragr...\n",
       "4  {'title': 'Victoria_(Avstralija)', 'paragraphs..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad = pd.read_json('../data/test2.json')\n",
    "del squad['version']\n",
    "squad.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_squad(data):\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    # iterate through all data in squad data\n",
    "    for _, group in data.iterrows():\n",
    "        for passage in group['data']['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                if 'plausible_answers' in qa.keys():\n",
    "                    access = 'plausible_answers'\n",
    "                else:\n",
    "                    access = 'answers'\n",
    "                for answer in qa[access]:\n",
    "                    # append data to lists\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "    # return formatted data lists\n",
    "    return contexts, questions, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts, train_questions, train_answers = read_squad(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Francija', 'answer_start': 147, 'answer_end': 155},\n",
       " {'text': 'Francija', 'answer_start': 147, 'answer_end': 155},\n",
       " {'text': 'Francija', 'answer_start': 147, 'answer_end': 155},\n",
       " {'text': 'Francija', 'answer_start': 147, 'answer_end': 155},\n",
       " {'text': '10. in 11. stoletje', 'answer_start': 97, 'answer_end': 116},\n",
       " {'text': 'v 10. in 11. stoletju', 'answer_start': 95, 'answer_end': 116},\n",
       " {'text': '10. in 11. stoletje', 'answer_start': 97, 'answer_end': 116},\n",
       " {'text': '10. in 11. stoletje', 'answer_start': 97, 'answer_end': 116},\n",
       " {'text': 'Danska, Islandija in Norveška',\n",
       "  'answer_start': 220,\n",
       "  'answer_end': 249},\n",
       " {'text': 'Danska, Islandija in Norveška',\n",
       "  'answer_start': 220,\n",
       "  'answer_end': 249}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_answers[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer_qa = BertTokenizerFast.from_pretrained('../data/CroSloEngual_BERT')\n",
    "# tokenizer_qa = BertTokenizer.from_pretrained('../data/CroSloEngual_BERT/vocab.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer_qa(train_contexts, train_questions, truncation=True, padding='max_length', max_length=512, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] normani ( norman : nourmands ; francoscina : normandi ; latinscina : normanni ) so bili ljudje, ki so v 10. in 11. stoletju dali ime normandiji, regiji v franciji. bili so potomci nordijskih ( \" norman \" ) napadalcev in piratov iz danske, islandije in norveske, ki so se pod svojim voditeljem rollom strinjali, da bodo prisegli zvestobo kralju karlu iii. iz zahodne frankovske. skozi generacije asimilacije in mesanja z domacimi frankovskimi in rimsko - gavskimi populacijami so se njihovi potomci postopoma zdruzili s karolinskimi kulturami zahodne frankovske. posebna kulturna in etnicna identiteta normanov se je najprej pojavila v prvi polovici 10. stoletja in se je razvijala v naslednjih stoletjih. [SEP] iz katerih drzav je norveska izvirala? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_qa.decode(train_encodings['input_ids'][10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to convert the character start and end position into token start and end position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    # initialize lists to contain the token indices of answer start/end\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    invalid = []\n",
    "    for i in range(len(answers)):\n",
    "        # append start/end token position using char_to_token method\n",
    "        start = encodings.char_to_token(i, answers[i]['answer_start'])\n",
    "        end = encodings.char_to_token(i, answers[i]['answer_start'])\n",
    "        cond = True\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start is None:\n",
    "            start = tokenizer_qa.model_max_length\n",
    "            invalid.append(i)\n",
    "            cond = False\n",
    "        # end position cannot be found, char_to_token found space, so shift one token forward\n",
    "        if cond:\n",
    "            go_back = 1\n",
    "            while end is None:\n",
    "                end = encodings.char_to_token(i, answers[i]['answer_end']-go_back)\n",
    "                go_back +=1\n",
    "            start_positions.append(start)\n",
    "            end_positions.append(end)\n",
    "    # update our encodings object with the new token-based start/end positions\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "    return invalid\n",
    "\n",
    "# apply function to our data\n",
    "invalid = add_token_positions(train_encodings, train_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = ['input_ids', 'token_type_ids', 'attention_mask']\n",
    "for key in keys:\n",
    "    tmp = np.delete(train_encodings[key], invalid, 0)\n",
    "    train_encodings.update({key: tmp})\n",
    "train_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            a = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        except RuntimeError:\n",
    "            for k, v in self.encodings.items():\n",
    "                print(f'{k}: {v}')\n",
    "            a = None\n",
    "        return a\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "# build datasets for both our training data\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "loader = torch.utils.data.DataLoader(train_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../data/CroSloEngual_BERT were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at ../data/CroSloEngual_BERT and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "model = BertForQuestionAnswering.from_pretrained('../data/CroSloEngual_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python3-8-10\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "optim = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5006 [00:00<?, ?it/s]C:\\Users\\Nace\\AppData\\Local\\Temp\\ipykernel_9120\\2171318946.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "Epoch 0: 100%|██████████| 5006/5006 [37:05<00:00,  2.25it/s, loss=3.04]  \n",
      "Epoch 1: 100%|██████████| 5006/5006 [36:08<00:00,  2.31it/s, loss=2.41]  \n",
      "Epoch 2: 100%|██████████| 5006/5006 [32:47<00:00,  2.54it/s, loss=0.518]  \n"
     ]
    }
   ],
   "source": [
    "for epoch in range(3):\n",
    "    loop = tqdm(loader)\n",
    "    for batch in loop:\n",
    "        optim.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        start_positions=start_positions,\n",
    "                        end_positions=end_positions)\n",
    "        \n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.mkdir('../data/bert_qa')\n",
    "\n",
    "torch.save(model.state_dict(), '../data/bert_qa/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(question, text):\n",
    "    # tokenize question and text as a pair\n",
    "    input_ids = tokenizer_qa.encode(question, text)\n",
    "    \n",
    "    # string version of tokenized ids\n",
    "    tokens = tokenizer_qa.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    # segment IDs\n",
    "    # first occurrence of [SEP] token\n",
    "    sep_idx = input_ids.index(tokenizer_qa.sep_token_id)\n",
    "    # number of tokens in segment A (question)\n",
    "    num_seg_a = sep_idx+1\n",
    "    # number of tokens in segment B (text)\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    \n",
    "    # list of 0s and 1s for segment embeddings\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "    \n",
    "    # model output using input_ids and segment_ids\n",
    "    output = model(torch.tensor([input_ids]).to(device), token_type_ids=torch.tensor([segment_ids]).to(device))\n",
    "    \n",
    "    # reconstructing the answer\n",
    "    answer_start = torch.argmax(output.start_logits)\n",
    "    answer_end = torch.argmax(output.end_logits) + 1 # needs to be changed down\n",
    "    if answer_end >= answer_start:\n",
    "        answer = tokens[answer_start]\n",
    "        for i in range(answer_start+1, answer_end+1): # here\n",
    "            if tokens[i][0:2] == \"##\":\n",
    "                answer += tokens[i][2:]\n",
    "            else:\n",
    "                answer += \" \" + tokens[i]\n",
    "                \n",
    "    if answer.startswith(\"[CLS]\"):\n",
    "        answer = \"Unable to find the answer to your question.\"\n",
    "    \n",
    "    return \"{}\".format(answer.capitalize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slovenija je imela nadpovprečno visoko gospodarsko rast, zgodovinsko najvišjo zaposlenost in kljub temu podpovprečno inflacijo. Izredno nizko stopnjo inflacije je beležila še prejšnji mesec, vendar so se številke z aprilom usmerile v nasprotno smer, kar pa v ljudeh vzbuja dvom in nezaupanje v prihodnjo vlado, da bo Slovenijo lahko peljala v napredek tako, kot je to uspevalo vladi Janeza Janše.\n",
      "\n",
      "\n",
      "1. Kaksno rast ima Slovenija?          Visoko gospodarsko.\n",
      "\n",
      "2. Kaksno gospodarsko rast je imela Slovenija?          Nadpov.\n",
      "\n",
      "3. Kaj je imela Slovenija?          Imela nad.\n",
      "\n",
      "4. Kaksna je bila inflacija?          Izredno nizko.\n",
      "\n",
      "5. Kdaj je belezila izredno nisko stopnjo inflacije?          Prejsn.\n",
      "\n",
      "6. Kam so se stevilke usmirile?          Z april.\n",
      "\n",
      "7. V kom vzbuja dvom?          Dvom in.\n",
      "\n",
      "8. Komu ljudje ne zaupajo?          V ljudeh.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "text = \"Slovenija je imela nadpovprečno visoko gospodarsko rast, zgodovinsko najvišjo zaposlenost in kljub temu podpovprečno inflacijo. Izredno nizko stopnjo inflacije je beležila še prejšnji mesec, vendar so se številke z aprilom usmerile v nasprotno smer, kar pa v ljudeh vzbuja dvom in nezaupanje v prihodnjo vlado, da bo Slovenijo lahko peljala v napredek tako, kot je to uspevalo vladi Janeza Janše.\"\n",
    "print(f\"{text}\\n\\n\")\n",
    "\n",
    "questions = [\"Kaksno rast ima Slovenija?\", \"Kaksno gospodarsko rast je imela Slovenija?\", \"Kaj je imela Slovenija?\", \"Kaksna je bila inflacija?\", \"Kdaj je belezila izredno nisko stopnjo inflacije?\", \"Kam so se stevilke usmirile?\", \"V kom vzbuja dvom?\", \"Komu ljudje ne zaupajo?\"]\n",
    "for count, question in enumerate(questions):\n",
    "    predicted_answer = question_answer(question, text)\n",
    "    print(f\"{count + 1}. {question}          {predicted_answer}.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c4a5c4f67b56b1967935a6e7ad4a45a7bb6af7379fc6f17e708100bf9a0d273d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
