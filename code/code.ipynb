{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# QA BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned BERT\n",
    "\n",
    "https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'title': 'Normans', 'paragraphs': [{'qas': [{...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'title': 'Computational_complexity_theory', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'title': 'Southern_California', 'paragraphs':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'title': 'Sky_(United_Kingdom)', 'paragraphs'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'title': 'Victoria_(Australia)', 'paragraphs'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data\n",
       "0  {'title': 'Normans', 'paragraphs': [{'qas': [{...\n",
       "1  {'title': 'Computational_complexity_theory', '...\n",
       "2  {'title': 'Southern_California', 'paragraphs':...\n",
       "3  {'title': 'Sky_(United_Kingdom)', 'paragraphs'...\n",
       "4  {'title': 'Victoria_(Australia)', 'paragraphs'..."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad = pd.read_json('../data/dev-v2.0.json')\n",
    "del squad['version']\n",
    "squad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(data):\n",
    "    # require columns in our dataframe\n",
    "    cols = ['text', 'question', 'answers']\n",
    "\n",
    "    # list of lists to create our dataframe\n",
    "    comp_list = []\n",
    "    i = 0\n",
    "    for _, dset in data.iterrows():\n",
    "        for row in dset['data']['paragraphs']:\n",
    "            for qas in row['qas']:\n",
    "                temp_list = []\n",
    "                temp_list.append(row['context'])\n",
    "                temp_list.append(qas['question'])\n",
    "                temp_list.append([a['text'] for a in qas['answers']])\n",
    "                comp_list.append(temp_list)\n",
    "    return pd.DataFrame(comp_list, columns=cols)\n",
    "\n",
    "df = get_dataframe(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions and answers: 11873\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>[France, France, France, France]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>When were the Normans in Normandy?</td>\n",
       "      <td>[10th and 11th centuries, in the 10th and 11th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>From which countries did the Norse originate?</td>\n",
       "      <td>[Denmark, Iceland and Norway, Denmark, Iceland...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>Who was the Norse leader?</td>\n",
       "      <td>[Rollo, Rollo, Rollo, Rollo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>What century did the Normans first gain their ...</td>\n",
       "      <td>[10th century, the first half of the 10th cent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The Normans (Norman: Nourmands; French: Norman...   \n",
       "1  The Normans (Norman: Nourmands; French: Norman...   \n",
       "2  The Normans (Norman: Nourmands; French: Norman...   \n",
       "3  The Normans (Norman: Nourmands; French: Norman...   \n",
       "4  The Normans (Norman: Nourmands; French: Norman...   \n",
       "\n",
       "                                            question  \\\n",
       "0               In what country is Normandy located?   \n",
       "1                 When were the Normans in Normandy?   \n",
       "2      From which countries did the Norse originate?   \n",
       "3                          Who was the Norse leader?   \n",
       "4  What century did the Normans first gain their ...   \n",
       "\n",
       "                                             answers  \n",
       "0                   [France, France, France, France]  \n",
       "1  [10th and 11th centuries, in the 10th and 11th...  \n",
       "2  [Denmark, Iceland and Norway, Denmark, Iceland...  \n",
       "3                       [Rollo, Rollo, Rollo, Rollo]  \n",
       "4  [10th century, the first half of the 10th cent...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of questions and answers: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine_tuned = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer_fine_tuned = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking a random question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_n = np.random.randint(0, len(df))\n",
    "\n",
    "question = df['question'][rand_n]\n",
    "text = df['text'][rand_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of the question and text as a pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 130 tokens.\n",
      " [CLS]                       101\n",
      " how                       2,129\n",
      " many                      2,116\n",
      " years                     2,086\n",
      " have                      2,031\n",
      " imperial                  4,461\n",
      " ##istic                   6,553\n",
      " practices                 6,078\n",
      " existed                   5,839\n",
      " ?                         1,029\n",
      " [SEP]                       102\n",
      " the                       1,996\n",
      " age                       2,287\n",
      " of                        1,997\n",
      " imperialism              28,087\n",
      " ,                         1,010\n",
      " a                         1,037\n",
      " time                      2,051\n",
      " period                    2,558\n",
      " beginning                 2,927\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer_fine_tuned.encode(question, text)\n",
    "print(f\"The input has a total of {len(input_ids)} tokens.\")\n",
    "\n",
    "tokens = tokenizer_fine_tuned.convert_ids_to_tokens(input_ids)\n",
    "count = 0\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    if count >= 20:\n",
    "        break\n",
    "    count += 1\n",
    "    print(f\" {token:15} {id:15,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment and position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP] token index: 10\n",
      "Numbers of tokens in segment A: 11\n",
      "Numbers of tokens in segment B: 119\n"
     ]
    }
   ],
   "source": [
    "# first occurrence of [SEP] token\n",
    "sep_idx = input_ids.index(tokenizer_fine_tuned.sep_token_id)\n",
    "print(f\"[SEP] token index: {sep_idx}\")\n",
    "\n",
    "# number of tokens in segment A (question)\n",
    "# this will be one more than the sep_idx as the index in Python starts from 0\n",
    "num_seg_a = sep_idx + 1\n",
    "print(f\"Numbers of tokens in segment A: {num_seg_a}\")\n",
    "\n",
    "# number of tokens in segment B (text)\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "print(f\"Numbers of tokens in segment B: {num_seg_b}\")\n",
    "\n",
    "# creating the segment ids\n",
    "segment_ids = [0] * num_seg_a + [1] * num_seg_b\n",
    "\n",
    "# making sure that every input token has a segment id\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding this to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "How many years have imperialistic practices existed?\n",
      "\n",
      "Answer:\n",
      "Thousands of years.\n"
     ]
    }
   ],
   "source": [
    "# token input_ids to represent the input and token segment_ids to differentiate our segments - question and text\n",
    "output = model_fine_tuned(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids])) \n",
    "\n",
    "# tokens with highest start and end scores\n",
    "answer_start = torch.argmax(output.start_logits)\n",
    "answer_end = torch.argmax(output.end_logits)\n",
    "\n",
    "if answer_end >= answer_start:\n",
    "    answer = tokens[answer_start]\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        if tokens[i][0:2] == \"##\":\n",
    "            answer += tokens[i][2:]\n",
    "        else:\n",
    "            answer += \" \" + tokens[i]\n",
    "else:\n",
    "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
    "\n",
    "print(f\"\\nQuestion:\\n{question.capitalize()}\")\n",
    "print(f\"\\nAnswer:\\n{answer.capitalize()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now turn this process into function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(question, text):\n",
    "    # tokenize question and text as a pair\n",
    "    input_ids = tokenizer_fine_tuned.encode(question, text)\n",
    "    \n",
    "    # string version of tokenized ids\n",
    "    tokens = tokenizer_fine_tuned.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    # segment IDs\n",
    "    # first occurrence of [SEP] token\n",
    "    sep_idx = input_ids.index(tokenizer_fine_tuned.sep_token_id)\n",
    "    # number of tokens in segment A (question)\n",
    "    num_seg_a = sep_idx+1\n",
    "    # number of tokens in segment B (text)\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    \n",
    "    # list of 0s and 1s for segment embeddings\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "    \n",
    "    # model output using input_ids and segment_ids\n",
    "    output = model_fine_tuned(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
    "    \n",
    "    # reconstructing the answer\n",
    "    answer_start = torch.argmax(output.start_logits)\n",
    "    answer_end = torch.argmax(output.end_logits)\n",
    "    if answer_end >= answer_start:\n",
    "        answer = tokens[answer_start]\n",
    "        for i in range(answer_start+1, answer_end+1):\n",
    "            if tokens[i][0:2] == \"##\":\n",
    "                answer += tokens[i][2:]\n",
    "            else:\n",
    "                answer += \" \" + tokens[i]\n",
    "                \n",
    "    if answer.startswith(\"[CLS]\"):\n",
    "        answer = \"Unable to find the answer to your question.\"\n",
    "    \n",
    "    print(\"\\nPredicted answer:\\n{}\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model using different text and question (not from our dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted answer:\n",
      "Hard rock cafe in new york ' s times square\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"New York (CNN) -- More than 80 Michael Jackson collectibles -- including the late pop star's famous rhinestone-studded glove from a 1983 performance -- were auctioned off Saturday, reaping a total $2 million. Profits from the auction at the Hard Rock Cafe in New York's Times Square crushed pre-sale expectations of only $120,000 in sales. The highly prized memorabilia, which included items spanning the many stages of Jackson's career, came from more than 30 fans, associates and family members, who contacted Julien's Auctions to sell their gifts and mementos of the singer. Jackson's flashy glove was the big-ticket item of the night, fetching $420,000 from a buyer in Hong Kong, China. Jackson wore the glove at a 1983 performance during \\\"Motown 25,\\\" an NBC special where he debuted his revolutionary moonwalk. Fellow Motown star Walter \\\"Clyde\\\" Orange of the Commodores, who also performed in the special 26 years ago, said he asked for Jackson's autograph at the time, but Jackson gave him the glove instead. \"The legacy that [Jackson] left behind is bigger than life for me,\\\" Orange said. \\\"I hope that through that glove people can see what he was trying to say in his music and what he said in his music.\\\" Orange said he plans to give a portion of the proceeds to charity. Hoffman Ma, who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyer's premium, which was tacked onto all final sales over $50,000. Winners of items less than $50,000 paid a 20 percent premium.\"\"\"\n",
    "question = \"Where was the Auction held?\"\n",
    "question_answer(question, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Train\n",
    "\n",
    "https://towardsdatascience.com/how-to-train-bert-for-q-a-in-any-language-63b62c780014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "https://towardsdatascience.com/how-to-build-a-wordpiece-tokenizer-for-bert-f505d97dddbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import datasets\\ndataset = datasets.load_dataset('oscar', 'unshuffled_deduplicated_sl')\\ndataset = dataset['train']\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import datasets\n",
    "dataset = datasets.load_dataset('oscar', 'unshuffled_deduplicated_sl')\n",
    "dataset = dataset['train']'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformating data into simple plaintext files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\ntext_data = []\\nfile_count = 0\\n\\nfor sample in tqdm(dataset):\\n    # remove newline characters from each sample as we need to use exclusively as seperators\\n    sample = sample['text'].replace('\\n', '\\\\s')\\n    text_data.append(sample)\\n    if len(text_data) == 5_000:\\n        # once we hit the 5K mark, save to file\\n        with open(f'../data/oscar_sl/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\\n            fp.write('\\n'.join(text_data))\\n        text_data = []\\n        file_count += 1\\n# after saving in 5K chunks, we may have leftover samples, we save those now too\\nwith open(f'../data/oscar_sl/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\\n    fp.write('\\n'.join(text_data))\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(dataset):\n",
    "    # remove newline characters from each sample as we need to use exclusively as seperators\n",
    "    sample = sample['text'].replace('\\n', '\\s')\n",
    "    text_data.append(sample)\n",
    "    if len(text_data) == 5_000:\n",
    "        # once we hit the 5K mark, save to file\n",
    "        with open(f'../data/oscar_sl/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "# after saving in 5K chunks, we may have leftover samples, we save those now too\n",
    "with open(f'../data/oscar_sl/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['..\\\\data\\\\oscar_sl\\\\text_0.txt',\n",
       " '..\\\\data\\\\oscar_sl\\\\text_1.txt',\n",
       " '..\\\\data\\\\oscar_sl\\\\text_10.txt',\n",
       " '..\\\\data\\\\oscar_sl\\\\text_100.txt',\n",
       " '..\\\\data\\\\oscar_sl\\\\text_101.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path('../data/oscar_sl').glob('**/*.txt')]\n",
    "print(len(paths))\n",
    "paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# initialize\\ntokenizer = BertWordPieceTokenizer(\\n    clean_text=True,\\n    handle_chinese_chars=False,\\n    strip_accents=False,\\n    lowercase=False\\n)\\n# and train\\ntokenizer.train(files=paths, vocab_size=100_000, min_frequency=2,\\n                limit_alphabet=1000, wordpieces_prefix='##',\\n                special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# initialize\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=False\n",
    ")\n",
    "# and train\n",
    "tokenizer.train(files=paths, vocab_size=100_000, min_frequency=2,\n",
    "                limit_alphabet=1000, wordpieces_prefix='##',\n",
    "                special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nace\\Documents\\MAG2\\NLP\\code\\code.ipynb Cell 30'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000056?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizerFast\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000056?line=2'>3</a>\u001b[0m \u001b[39m# initialize\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000056?line=3'>4</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m BertTokenizerFast(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000056?line=4'>5</a>\u001b[0m     clean_text\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000056?line=5'>6</a>\u001b[0m     handle_chinese_chars\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000056?line=6'>7</a>\u001b[0m     strip_accents\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000056?line=7'>8</a>\u001b[0m     lowercase\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000056?line=8'>9</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000056?line=9'>10</a>\u001b[0m \u001b[39m# and train\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000056?line=10'>11</a>\u001b[0m tokenizer\u001b[39m.\u001b[39mtrain(files\u001b[39m=\u001b[39mpaths, vocab_size\u001b[39m=\u001b[39m\u001b[39m100_000\u001b[39m, min_frequency\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000056?line=11'>12</a>\u001b[0m                 limit_alphabet\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m, wordpieces_prefix\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m##\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000056?line=12'>13</a>\u001b[0m                 special_tokens\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m[PAD]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m[UNK]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m[CLS]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m[SEP]\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m[MASK]\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert_fast.py:177\u001b[0m, in \u001b[0;36mBertTokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, do_lower_case, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=162'>163</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=163'>164</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=164'>165</a>\u001b[0m     vocab_file\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=174'>175</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=175'>176</a>\u001b[0m ):\n\u001b[1;32m--> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=176'>177</a>\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=177'>178</a>\u001b[0m         vocab_file,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=178'>179</a>\u001b[0m         tokenizer_file\u001b[39m=\u001b[39;49mtokenizer_file,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=179'>180</a>\u001b[0m         do_lower_case\u001b[39m=\u001b[39;49mdo_lower_case,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=180'>181</a>\u001b[0m         unk_token\u001b[39m=\u001b[39;49munk_token,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=181'>182</a>\u001b[0m         sep_token\u001b[39m=\u001b[39;49msep_token,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=182'>183</a>\u001b[0m         pad_token\u001b[39m=\u001b[39;49mpad_token,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=183'>184</a>\u001b[0m         cls_token\u001b[39m=\u001b[39;49mcls_token,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=184'>185</a>\u001b[0m         mask_token\u001b[39m=\u001b[39;49mmask_token,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=185'>186</a>\u001b[0m         tokenize_chinese_chars\u001b[39m=\u001b[39;49mtokenize_chinese_chars,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=186'>187</a>\u001b[0m         strip_accents\u001b[39m=\u001b[39;49mstrip_accents,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=187'>188</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=188'>189</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=190'>191</a>\u001b[0m     normalizer_state \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbackend_tokenizer\u001b[39m.\u001b[39mnormalizer\u001b[39m.\u001b[39m__getstate__())\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=191'>192</a>\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=192'>193</a>\u001b[0m         normalizer_state\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m\"\u001b[39m, do_lower_case) \u001b[39m!=\u001b[39m do_lower_case\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=193'>194</a>\u001b[0m         \u001b[39mor\u001b[39;00m normalizer_state\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstrip_accents\u001b[39m\u001b[39m\"\u001b[39m, strip_accents) \u001b[39m!=\u001b[39m strip_accents\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=194'>195</a>\u001b[0m         \u001b[39mor\u001b[39;00m normalizer_state\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mhandle_chinese_chars\u001b[39m\u001b[39m\"\u001b[39m, tokenize_chinese_chars) \u001b[39m!=\u001b[39m tokenize_chinese_chars\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert_fast.py?line=195'>196</a>\u001b[0m     ):\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:115\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/tokenization_utils_fast.py?line=111'>112</a>\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/tokenization_utils_fast.py?line=112'>113</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslow_tokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/tokenization_utils_fast.py?line=113'>114</a>\u001b[0m     \u001b[39m# We need to create and convert a slow tokenizer to build the backend\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/tokenization_utils_fast.py?line=114'>115</a>\u001b[0m     slow_tokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslow_tokenizer_class(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/tokenization_utils_fast.py?line=115'>116</a>\u001b[0m     fast_tokenizer \u001b[39m=\u001b[39m convert_slow_tokenizer(slow_tokenizer)\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/tokenization_utils_fast.py?line=116'>117</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\transformers\\models\\bert\\tokenization_bert.py:193\u001b[0m, in \u001b[0;36mBertTokenizer.__init__\u001b[1;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=163'>164</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=164'>165</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=165'>166</a>\u001b[0m     vocab_file,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=176'>177</a>\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=177'>178</a>\u001b[0m ):\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=178'>179</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=179'>180</a>\u001b[0m         do_lower_case\u001b[39m=\u001b[39mdo_lower_case,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=180'>181</a>\u001b[0m         do_basic_tokenize\u001b[39m=\u001b[39mdo_basic_tokenize,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=189'>190</a>\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=190'>191</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=192'>193</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49misfile(vocab_file):\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=193'>194</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=194'>195</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCan\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a vocabulary file at path \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mvocab_file\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m. To load the vocabulary from a Google pretrained \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=195'>196</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmodel use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=196'>197</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/tokenization_bert.py?line=197'>198</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab \u001b[39m=\u001b[39m load_vocab(vocab_file)\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Python/Python3-8-10/lib/genericpath.py?line=27'>28</a>\u001b[0m \u001b[39m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Python/Python3-8-10/lib/genericpath.py?line=28'>29</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Python/Python3-8-10/lib/genericpath.py?line=29'>30</a>\u001b[0m     st \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mstat(path)\n\u001b[0;32m     <a href='file:///c%3A/Python/Python3-8-10/lib/genericpath.py?line=30'>31</a>\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mOSError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[0;32m     <a href='file:///c%3A/Python/Python3-8-10/lib/genericpath.py?line=31'>32</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# initialize\n",
    "tokenizer = BertTokenizerFast(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=False\n",
    ")\n",
    "# and train\n",
    "tokenizer.train(files=paths, vocab_size=100_000, min_frequency=2,\n",
    "                limit_alphabet=1000, wordpieces_prefix='##',\n",
    "                special_tokens=['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nimport os\\n\\nos.mkdir('../data/bert_sl')\\ntokenizer.save_model('../data/bert_sl', 'sl')\""
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save tokenizer\n",
    "'''\n",
    "import os\n",
    "\n",
    "os.mkdir('../data/bert_sl')\n",
    "tokenizer.save_model('../data/bert_sl', 'sl')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Python3-8-10\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1653: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('../data/bert_sl/sl-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/bert_sl/sl-vocab.txt', 'r', encoding='utf-8') as fp:\n",
    "    vocab = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [CLS]\n",
      "4084 tukaj\n",
      "2039 lahko\n",
      "5407 uporabnik\n",
      "4 [MASK]\n",
      "54542 napise\n",
      "63277 poljuben\n",
      "18591 stavek\n",
      "90 v\n",
      "0 [PAD]\n",
      "5961 sloven\n",
      "14307 ##sci\n",
      "1935 ##ni\n",
      "18 .\n",
      "3 [SEP]\n"
     ]
    }
   ],
   "source": [
    "stavek = 'Tukaj lahko uporabnik [MASK] napiše poljuben stavek v [PAD] slovenščini.'\n",
    "tokens = tokenizer(stavek)['input_ids']\n",
    "for t in tokens:\n",
    "    print(str(t) + \" \" + vocab[t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT sl\n",
    "\n",
    "https://towardsdatascience.com/how-to-train-bert-for-q-a-in-any-language-63b62c780014"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT pretrain (MLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntext = []\\nfor p in paths[:1]:\\n    with open(p, 'r',encoding='utf-8') as f:\\n        tmp = re.split(r'[\\n,\\\\s]', f.read())\\n        if type(tmp) == list:\\n            for t in tmp:\\n                tokenize.sent_tokenize(t)\\n        else:\\n            text.append(tmp)\\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk import tokenize, download\n",
    "\"\"\"\n",
    "text = []\n",
    "for p in paths[:1]:\n",
    "    with open(p, 'r',encoding='utf-8') as f:\n",
    "        tmp = re.split(r'[\\n,\\s]', f.read())\n",
    "        if type(tmp) == list:\n",
    "            for t in tmp:\n",
    "                tokenize.sent_tokenize(t)\n",
    "        else:\n",
    "            text.append(tmp)\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_data = []\n",
    "for p in paths[:1]:\n",
    "    with open(p, 'r',encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            line = line.replace('\\n', '').replace('\\\\s', ' ').replace('\\s', ' ')\n",
    "            mlm_data.append(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForMaskedLM, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'title': 'Normani', 'paragraphs': [{'qas': [{...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'title': 'Computational_complexity_theory', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'title': 'Southern_California', 'paragraphs':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'title': 'Sky_(Združeno kraljestvo)', 'paragr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'title': 'Victoria_(Avstralija)', 'paragraphs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data\n",
       "0  {'title': 'Normani', 'paragraphs': [{'qas': [{...\n",
       "1  {'title': 'Computational_complexity_theory', '...\n",
       "2  {'title': 'Southern_California', 'paragraphs':...\n",
       "3  {'title': 'Sky_(Združeno kraljestvo)', 'paragr...\n",
       "4  {'title': 'Victoria_(Avstralija)', 'paragraphs..."
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_sl = pd.read_json('./export-test.json')\n",
    "del squad_sl['version']\n",
    "squad_sl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>[(159, France), (159, France), (159, France), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>When were the Normans in Normandy?</td>\n",
       "      <td>[(94, 10th and 11th centuries), (87, in the 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>From which countries did the Norse originate?</td>\n",
       "      <td>[(256, Denmark, Iceland and Norway), (256, Den...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>Who was the Norse leader?</td>\n",
       "      <td>[(308, Rollo), (308, Rollo), (308, Rollo), (30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>What century did the Normans first gain their ...</td>\n",
       "      <td>[(671, 10th century), (649, the first half of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11868</th>\n",
       "      <td>The pound-force has a metric counterpart, less...</td>\n",
       "      <td>What is the seldom used force unit equal to on...</td>\n",
       "      <td>[(665, sthène), (665, sthène), (665, sthène), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11869</th>\n",
       "      <td>The pound-force has a metric counterpart, less...</td>\n",
       "      <td>What does not have a metric counterpart?</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11870</th>\n",
       "      <td>The pound-force has a metric counterpart, less...</td>\n",
       "      <td>What is the force exerted by standard gravity ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11871</th>\n",
       "      <td>The pound-force has a metric counterpart, less...</td>\n",
       "      <td>What force leads to a commonly used unit of mass?</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11872</th>\n",
       "      <td>The pound-force has a metric counterpart, less...</td>\n",
       "      <td>What force is part of the modern SI system?</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11873 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      The Normans (Norman: Nourmands; French: Norman...   \n",
       "1      The Normans (Norman: Nourmands; French: Norman...   \n",
       "2      The Normans (Norman: Nourmands; French: Norman...   \n",
       "3      The Normans (Norman: Nourmands; French: Norman...   \n",
       "4      The Normans (Norman: Nourmands; French: Norman...   \n",
       "...                                                  ...   \n",
       "11868  The pound-force has a metric counterpart, less...   \n",
       "11869  The pound-force has a metric counterpart, less...   \n",
       "11870  The pound-force has a metric counterpart, less...   \n",
       "11871  The pound-force has a metric counterpart, less...   \n",
       "11872  The pound-force has a metric counterpart, less...   \n",
       "\n",
       "                                                question  \\\n",
       "0                   In what country is Normandy located?   \n",
       "1                     When were the Normans in Normandy?   \n",
       "2          From which countries did the Norse originate?   \n",
       "3                              Who was the Norse leader?   \n",
       "4      What century did the Normans first gain their ...   \n",
       "...                                                  ...   \n",
       "11868  What is the seldom used force unit equal to on...   \n",
       "11869           What does not have a metric counterpart?   \n",
       "11870  What is the force exerted by standard gravity ...   \n",
       "11871  What force leads to a commonly used unit of mass?   \n",
       "11872        What force is part of the modern SI system?   \n",
       "\n",
       "                                                 answers  \n",
       "0      [(159, France), (159, France), (159, France), ...  \n",
       "1      [(94, 10th and 11th centuries), (87, in the 10...  \n",
       "2      [(256, Denmark, Iceland and Norway), (256, Den...  \n",
       "3      [(308, Rollo), (308, Rollo), (308, Rollo), (30...  \n",
       "4      [(671, 10th century), (649, the first half of ...  \n",
       "...                                                  ...  \n",
       "11868  [(665, sthène), (665, sthène), (665, sthène), ...  \n",
       "11869                                                 []  \n",
       "11870                                                 []  \n",
       "11871                                                 []  \n",
       "11872                                                 []  \n",
       "\n",
       "[11873 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_data(data):\n",
    "    # require columns in our dataframe\n",
    "    cols = ['text', 'question', 'answers']\n",
    "\n",
    "    # list of lists to create our dataframe\n",
    "    comp_list = []\n",
    "    i = 0\n",
    "    for _, dset in data.iterrows():\n",
    "        for row in dset['data']['paragraphs']:\n",
    "            for qas in row['qas']:\n",
    "                temp_list = []\n",
    "                temp_list.append(row['context'])\n",
    "                temp_list.append(qas['question'])\n",
    "                temp_list.append([(a['answer_start'], a['text']) for a in qas['answers']])\n",
    "                comp_list.append(temp_list)\n",
    "    return pd.DataFrame(comp_list, columns=cols)\n",
    "\n",
    "\n",
    "data = get_data(squad)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(mlm_data, return_tensors='pt', max_length=2048, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    2,  2099, 74072,  ...,     0,     0,     0],\n",
       "        [    2,  2041,  2563,  ...,     0,     0,     0],\n",
       "        [    2,  2460,  1954,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2, 44868,  1985,  ...,     0,     0,     0],\n",
       "        [    2,  3678, 15569,  ...,     0,     0,     0],\n",
       "        [    2,  6935, 53719,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[    2,  2099, 74072,  ...,     0,     0,     0],\n",
       "        [    2,  2041,  2563,  ...,     0,     0,     0],\n",
       "        [    2,  2460,  1954,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2, 44868,  1985,  ...,     0,     0,     0],\n",
       "        [    2,  3678, 15569,  ...,     0,     0,     0],\n",
       "        [    2,  6935, 53719,  ...,     0,     0,     0]])}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['labels'] = inputs.input_ids.detach().clone()\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        ...,\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False],\n",
       "        [False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 2) * (inputs.input_ids != 4) * (inputs.input_ids != 0) # we don't want to mask [CLS], [MASK] and [PAD] tokens\n",
    "mask_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(torch.flatten(mask_arr[i].nonzero()).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(mask_arr.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = 4 # 4 == [MASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  2099, 74072,  ...,     0,     0,     0],\n",
       "        [    2,  2041,  2563,  ...,     0,     0,     0],\n",
       "        [    2,  2460,  1954,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    2, 44868,  1985,  ...,     0,     0,     0],\n",
       "        [    2,  3678, 15569,  ...,     0,     0,     0],\n",
       "        [    2,  6935, 53719,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(inputs)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--disable-cuda]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9008 --control=9006 --hb=9005 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"1d982e7b-9042-4c94-a50b-82fc5b533d46\" --shell=9007 --transport=\"tcp\" --iopub=9009 --f=C:\\Users\\Nace\\AppData\\Local\\Temp\\tmp-1077669wGW0cwJsiQ.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "#device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "#device\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch Example')\n",
    "parser.add_argument('--disable-cuda', action='store_true',\n",
    "                    help='Disable CUDA')\n",
    "args = parser.parse_args()\n",
    "args.device = None\n",
    "if not args.disable_cuda and torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "else:\n",
    "    args.device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nace\\Documents\\MAG2\\NLP\\code\\code.ipynb Cell 53'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000052?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000052?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\torch\\nn\\modules\\module.py:899\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=894'>895</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=895'>896</a>\u001b[0m                     non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=896'>897</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m--> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=898'>899</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply(convert)\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\torch\\nn\\modules\\module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=568'>569</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=569'>570</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=571'>572</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=572'>573</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=573'>574</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=574'>575</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\torch\\nn\\modules\\module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=568'>569</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=569'>570</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=571'>572</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=572'>573</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=573'>574</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=574'>575</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\torch\\nn\\modules\\module.py:570\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=567'>568</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply\u001b[39m(\u001b[39mself\u001b[39m, fn):\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=568'>569</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchildren():\n\u001b[1;32m--> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=569'>570</a>\u001b[0m         module\u001b[39m.\u001b[39;49m_apply(fn)\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=571'>572</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=572'>573</a>\u001b[0m         \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=573'>574</a>\u001b[0m             \u001b[39m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=574'>575</a>\u001b[0m             \u001b[39m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=579'>580</a>\u001b[0m             \u001b[39m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=580'>581</a>\u001b[0m             \u001b[39m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\torch\\nn\\modules\\module.py:593\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=588'>589</a>\u001b[0m \u001b[39m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=589'>590</a>\u001b[0m \u001b[39m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=590'>591</a>\u001b[0m \u001b[39m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=591'>592</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=592'>593</a>\u001b[0m     param_applied \u001b[39m=\u001b[39m fn(param)\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=593'>594</a>\u001b[0m should_use_set_data \u001b[39m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=594'>595</a>\u001b[0m \u001b[39mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\torch\\nn\\modules\\module.py:897\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=893'>894</a>\u001b[0m \u001b[39mif\u001b[39;00m convert_to_format \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mdim() \u001b[39min\u001b[39;00m (\u001b[39m4\u001b[39m, \u001b[39m5\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=894'>895</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39mto(device, dtype \u001b[39mif\u001b[39;00m t\u001b[39m.\u001b[39mis_floating_point() \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mis_complex() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=895'>896</a>\u001b[0m                 non_blocking, memory_format\u001b[39m=\u001b[39mconvert_to_format)\n\u001b[1;32m--> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=896'>897</a>\u001b[0m \u001b[39mreturn\u001b[39;00m t\u001b[39m.\u001b[39;49mto(device, dtype \u001b[39mif\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_floating_point() \u001b[39mor\u001b[39;49;00m t\u001b[39m.\u001b[39;49mis_complex() \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m, non_blocking)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = AdamW(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nace\\Documents\\MAG2\\NLP\\code\\code.ipynb Cell 55'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000055?line=0'>1</a>\u001b[0m t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m])\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000055?line=1'>2</a>\u001b[0m t\u001b[39m.\u001b[39mget_device()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "t = torch.tensor([1,2]).to(device)\n",
    "t.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]C:\\Users\\Nace\\AppData\\Local\\Temp\\ipykernel_3612\\2201472103.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\Nace\\AppData\\Local\\Temp\\ipykernel_3612\\91515600.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(batch['input_ids'], device=device)\n",
      "C:\\Users\\Nace\\AppData\\Local\\Temp\\ipykernel_3612\\91515600.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(batch['attention_mask'], device=device)\n",
      "C:\\Users\\Nace\\AppData\\Local\\Temp\\ipykernel_3612\\91515600.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch['labels'], device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method size of Tensor object at 0x000001AF9181B360>\n",
      "<built-in method size of Tensor object at 0x000001AF9181B220>\n",
      "<built-in method size of Tensor object at 0x000001AF9181B310>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Nace\\Documents\\MAG2\\NLP\\code\\code.ipynb Cell 56'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000054?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m(attention_mask\u001b[39m.\u001b[39msize)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000054?line=11'>12</a>\u001b[0m \u001b[39mprint\u001b[39m(labels\u001b[39m.\u001b[39msize)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000054?line=13'>14</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(input_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, labels\u001b[39m=\u001b[39;49mlabels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000054?line=14'>15</a>\u001b[0m loss \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mloss\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Nace/Documents/MAG2/NLP/code/code.ipynb#ch0000054?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1343\u001b[0m, in \u001b[0;36mBertForMaskedLM.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1333'>1334</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1334'>1335</a>\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1335'>1336</a>\u001b[0m \u001b[39m    Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1336'>1337</a>\u001b[0m \u001b[39m    config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1337'>1338</a>\u001b[0m \u001b[39m    loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1338'>1339</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1340'>1341</a>\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1342'>1343</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1343'>1344</a>\u001b[0m     input_ids,\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1344'>1345</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1345'>1346</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1346'>1347</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1347'>1348</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1348'>1349</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1349'>1350</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1350'>1351</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1351'>1352</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1352'>1353</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1353'>1354</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1354'>1355</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1356'>1357</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1357'>1358</a>\u001b[0m prediction_scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls(sequence_output)\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:989\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=981'>982</a>\u001b[0m \u001b[39m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=982'>983</a>\u001b[0m \u001b[39m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=983'>984</a>\u001b[0m \u001b[39m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=984'>985</a>\u001b[0m \u001b[39m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=985'>986</a>\u001b[0m \u001b[39m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=986'>987</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=988'>989</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=989'>990</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=990'>991</a>\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=991'>992</a>\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=992'>993</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=993'>994</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39;49mpast_key_values_length,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=994'>995</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=995'>996</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=996'>997</a>\u001b[0m     embedding_output,\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=997'>998</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1005'>1006</a>\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1006'>1007</a>\u001b[0m )\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=1007'>1008</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:222\u001b[0m, in \u001b[0;36mBertEmbeddings.forward\u001b[1;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=219'>220</a>\u001b[0m     embeddings \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m position_embeddings\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=220'>221</a>\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(embeddings)\n\u001b[1;32m--> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=221'>222</a>\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout(embeddings)\n\u001b[0;32m    <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/transformers/models/bert/modeling_bert.py?line=222'>223</a>\u001b[0m \u001b[39mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/dropout.py?line=56'>57</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m---> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/modules/dropout.py?line=57'>58</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32mC:\\Python\\Python3-8-10\\lib\\site-packages\\torch\\nn\\functional.py:1169\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/functional.py?line=1166'>1167</a>\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[0;32m   <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/functional.py?line=1167'>1168</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[1;32m-> <a href='file:///c%3A/Python/Python3-8-10/lib/site-packages/torch/nn/functional.py?line=1168'>1169</a>\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "epochs = 2 # if number is large it can overtrain easily\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch in loop:\n",
    "        optim.zero_grad()\n",
    "        input_ids = torch.tensor(batch['input_ids'], device=device)\n",
    "        attention_mask = torch.tensor(batch['attention_mask'], device=device)\n",
    "        labels = torch.tensor(batch['labels'], device=device)\n",
    "        print(input_ids.size)\n",
    "        print(attention_mask.size)\n",
    "        print(labels.size)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = output.loss\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26602ab9730260b5ce0f6b9e387bf735a6962b6d9b4f101674e777f3a1c7823f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
