{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# QA BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuned BERT\n",
    "\n",
    "https://towardsdatascience.com/question-answering-with-a-fine-tuned-bert-bc4dafd45626"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertForQuestionAnswering, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'title': 'Normans', 'paragraphs': [{'qas': [{...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'title': 'Computational_complexity_theory', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'title': 'Southern_California', 'paragraphs':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'title': 'Sky_(United_Kingdom)', 'paragraphs'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'title': 'Victoria_(Australia)', 'paragraphs'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data\n",
       "0  {'title': 'Normans', 'paragraphs': [{'qas': [{...\n",
       "1  {'title': 'Computational_complexity_theory', '...\n",
       "2  {'title': 'Southern_California', 'paragraphs':...\n",
       "3  {'title': 'Sky_(United_Kingdom)', 'paragraphs'...\n",
       "4  {'title': 'Victoria_(Australia)', 'paragraphs'..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad = pd.read_json('..\\\\data\\\\dev-v2.0.json')\n",
    "del squad['version']\n",
    "squad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require columns in our dataframe\n",
    "cols = ['text', 'question', 'answers']\n",
    "\n",
    "# list of lists to create our dataframe\n",
    "comp_list = []\n",
    "i = 0\n",
    "for _, dset in squad.iterrows():\n",
    "    for row in dset['data']['paragraphs']:\n",
    "        for qas in row['qas']:\n",
    "            temp_list = []\n",
    "            temp_list.append(row['context'])\n",
    "            temp_list.append(qas['question'])\n",
    "            temp_list.append([a['text'] for a in qas['answers']])\n",
    "            comp_list.append(temp_list)\n",
    "df = pd.DataFrame(comp_list, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of questions and answers: 11873\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>In what country is Normandy located?</td>\n",
       "      <td>[France, France, France, France]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>When were the Normans in Normandy?</td>\n",
       "      <td>[10th and 11th centuries, in the 10th and 11th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>From which countries did the Norse originate?</td>\n",
       "      <td>[Denmark, Iceland and Norway, Denmark, Iceland...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>Who was the Norse leader?</td>\n",
       "      <td>[Rollo, Rollo, Rollo, Rollo]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Normans (Norman: Nourmands; French: Norman...</td>\n",
       "      <td>What century did the Normans first gain their ...</td>\n",
       "      <td>[10th century, the first half of the 10th cent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  ...                                            answers\n",
       "0  The Normans (Norman: Nourmands; French: Norman...  ...                   [France, France, France, France]\n",
       "1  The Normans (Norman: Nourmands; French: Norman...  ...  [10th and 11th centuries, in the 10th and 11th...\n",
       "2  The Normans (Norman: Nourmands; French: Norman...  ...  [Denmark, Iceland and Norway, Denmark, Iceland...\n",
       "3  The Normans (Norman: Nourmands; French: Norman...  ...                       [Rollo, Rollo, Rollo, Rollo]\n",
       "4  The Normans (Norman: Nourmands; French: Norman...  ...  [10th century, the first half of the 10th cent...\n",
       "\n",
       "[5 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Number of questions and answers: {len(df)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fine_tuned = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "tokenizer_fine_tuned = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking a random question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_n = np.random.randint(0, len(df))\n",
    "\n",
    "question = df['question'][rand_n]\n",
    "text = df['text'][rand_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization of the question and text as a pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 195 tokens.\n",
      " [CLS]                       101\n",
      " where                     2,073\n",
      " was                       2,001\n",
      " francis                   4,557\n",
      " he                        2,002\n",
      " ##is                      2,483\n",
      " ##ler                     3,917\n",
      " taken                     2,579\n",
      " after                     2,044\n",
      " the                       1,996\n",
      " protest                   6,186\n",
      " ?                         1,029\n",
      " [SEP]                       102\n",
      " when                      2,043\n",
      " the                       1,996\n",
      " committee                 2,837\n",
      " for                       2,005\n",
      " non                       2,512\n",
      " -                         1,011\n",
      " violent                   6,355\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer_fine_tuned.encode(question, text)\n",
    "print(f\"The input has a total of {len(input_ids)} tokens.\")\n",
    "\n",
    "tokens = tokenizer_fine_tuned.convert_ids_to_tokens(input_ids)\n",
    "count = 0\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    if count >= 20:\n",
    "        break\n",
    "    count += 1\n",
    "    print(f\" {token:15} {id:15,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment and position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP] token index: 12\n",
      "Numbers of tokens in segment A: 13\n",
      "Numbers of tokens in segment B: 182\n"
     ]
    }
   ],
   "source": [
    "# first occurrence of [SEP] token\n",
    "sep_idx = input_ids.index(tokenizer.sep_token_id)\n",
    "print(f\"[SEP] token index: {sep_idx}\")\n",
    "\n",
    "# number of tokens in segment A (question)\n",
    "# this will be one more than the sep_idx as the index in Python starts from 0\n",
    "num_seg_a = sep_idx + 1\n",
    "print(f\"Numbers of tokens in segment A: {num_seg_a}\")\n",
    "\n",
    "# number of tokens in segment B (text)\n",
    "num_seg_b = len(input_ids) - num_seg_a\n",
    "print(f\"Numbers of tokens in segment B: {num_seg_b}\")\n",
    "\n",
    "# creating the segment ids\n",
    "segment_ids = [0] * num_seg_a + [1] * num_seg_b\n",
    "\n",
    "# making sure that every input token has a segment id\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feeding this to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:\n",
      "Where was francis heisler taken after the protest?\n",
      "\n",
      "Answer:\n",
      "Tonopah , nevada.\n"
     ]
    }
   ],
   "source": [
    "# token input_ids to represent the input and token segment_ids to differentiate our segments - question and text\n",
    "output = model_fine_tuned(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids])) \n",
    "\n",
    "# tokens with highest start and end scores\n",
    "answer_start = torch.argmax(output.start_logits)\n",
    "answer_end = torch.argmax(output.end_logits)\n",
    "\n",
    "if answer_end >= answer_start:\n",
    "    answer = tokens[answer_start]\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        if tokens[i][0:2] == \"##\":\n",
    "            answer += tokens[i][2:]\n",
    "        else:\n",
    "            answer += \" \" + tokens[i]\n",
    "else:\n",
    "    print(\"I am unable to find the answer to this question. Can you please ask another question?\")\n",
    "\n",
    "print(f\"\\nQuestion:\\n{question.capitalize()}\")\n",
    "print(f\"\\nAnswer:\\n{answer.capitalize()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now turn this process into function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_answer(question, text):\n",
    "    # tokenize question and text as a pair\n",
    "    input_ids = tokenizer_fine_tuned.encode(question, text)\n",
    "    \n",
    "    # string version of tokenized ids\n",
    "    tokens = tokenizer_fine_tuned.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    # segment IDs\n",
    "    # first occurrence of [SEP] token\n",
    "    sep_idx = input_ids.index(tokenizer_fine_tuned.sep_token_id)\n",
    "    # number of tokens in segment A (question)\n",
    "    num_seg_a = sep_idx+1\n",
    "    # number of tokens in segment B (text)\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "    \n",
    "    # list of 0s and 1s for segment embeddings\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "    \n",
    "    # model output using input_ids and segment_ids\n",
    "    output = model_fine_tuned(torch.tensor([input_ids]), token_type_ids=torch.tensor([segment_ids]))\n",
    "    \n",
    "    # reconstructing the answer\n",
    "    answer_start = torch.argmax(output.start_logits)\n",
    "    answer_end = torch.argmax(output.end_logits)\n",
    "    if answer_end >= answer_start:\n",
    "        answer = tokens[answer_start]\n",
    "        for i in range(answer_start+1, answer_end+1):\n",
    "            if tokens[i][0:2] == \"##\":\n",
    "                answer += tokens[i][2:]\n",
    "            else:\n",
    "                answer += \" \" + tokens[i]\n",
    "                \n",
    "    if answer.startswith(\"[CLS]\"):\n",
    "        answer = \"Unable to find the answer to your question.\"\n",
    "    \n",
    "    print(\"\\nPredicted answer:\\n{}\".format(answer.capitalize()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test model using different text and question (not from our dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted answer:\n",
      "Hard rock cafe in new york ' s times square\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"New York (CNN) -- More than 80 Michael Jackson collectibles -- including the late pop star's famous rhinestone-studded glove from a 1983 performance -- were auctioned off Saturday, reaping a total $2 million. Profits from the auction at the Hard Rock Cafe in New York's Times Square crushed pre-sale expectations of only $120,000 in sales. The highly prized memorabilia, which included items spanning the many stages of Jackson's career, came from more than 30 fans, associates and family members, who contacted Julien's Auctions to sell their gifts and mementos of the singer. Jackson's flashy glove was the big-ticket item of the night, fetching $420,000 from a buyer in Hong Kong, China. Jackson wore the glove at a 1983 performance during \\\"Motown 25,\\\" an NBC special where he debuted his revolutionary moonwalk. Fellow Motown star Walter \\\"Clyde\\\" Orange of the Commodores, who also performed in the special 26 years ago, said he asked for Jackson's autograph at the time, but Jackson gave him the glove instead. \"The legacy that [Jackson] left behind is bigger than life for me,\\\" Orange said. \\\"I hope that through that glove people can see what he was trying to say in his music and what he said in his music.\\\" Orange said he plans to give a portion of the proceeds to charity. Hoffman Ma, who bought the glove on behalf of Ponte 16 Resort in Macau, paid a 25 percent buyer's premium, which was tacked onto all final sales over $50,000. Winners of items less than $50,000 paid a 20 percent premium.\"\"\"\n",
    "question = \"Where was the Auction held?\"\n",
    "question_answer(question, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "https://towardsdatascience.com/how-to-build-a-wordpiece-tokenizer-for-bert-f505d97dddbb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 14.8kB [00:00, 4.92MB/s]                   \n",
      "Downloading metadata: 3.07MB [00:00, 90.6MB/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset oscar/unshuffled_deduplicated_sl (download: 498.98 MiB, generated: 1.22 GiB, post-processed: Unknown size, total: 1.71 GiB) to C:\\Users\\Nace\\.cache\\huggingface\\datasets\\oscar\\unshuffled_deduplicated_sl\\1.0.0\\84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 81.0/81.0 [00:00<00:00, 80.9kB/s]\n",
      "Downloading data: 100%|██████████| 523M/523M [00:42<00:00, 12.4MB/s]\n",
      "Downloading data files: 100%|██████████| 1/1 [00:43<00:00, 43.26s/it]\n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset oscar downloaded and prepared to C:\\Users\\Nace\\.cache\\huggingface\\datasets\\oscar\\unshuffled_deduplicated_sl\\1.0.0\\84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  7.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "dataset = datasets.load_dataset('oscar', 'unshuffled_deduplicated_sl')\n",
    "dataset = dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformating data into simple plaintext files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 886223/886223 [00:50<00:00, 17505.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "text_data = []\n",
    "file_count = 0\n",
    "\n",
    "for sample in tqdm(dataset):\n",
    "    # remove newline characters from each sample as we need to use exclusively as seperators\n",
    "    sample = sample['text'].replace('\\n', '\\s')\n",
    "    text_data.append(sample)\n",
    "    if len(text_data) == 5_000:\n",
    "        # once we hit the 5K mark, save to file\n",
    "        with open(f'../data/oscar_sl/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "            fp.write('\\n'.join(text_data))\n",
    "        text_data = []\n",
    "        file_count += 1\n",
    "# after saving in 5K chunks, we may have leftover samples, we save those now too\n",
    "with open(f'../data/oscar_sl/text_{file_count}.txt', 'w', encoding='utf-8') as fp:\n",
    "    fp.write('\\n'.join(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['..\\\\data\\\\oscar_sl\\\\text_0.txt',\n",
       " '..\\\\data\\\\oscar_sl\\\\text_1.txt',\n",
       " '..\\\\data\\\\oscar_sl\\\\text_10.txt',\n",
       " '..\\\\data\\\\oscar_sl\\\\text_100.txt',\n",
       " '..\\\\data\\\\oscar_sl\\\\text_101.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "paths = [str(x) for x in Path('../data/oscar_sl').glob('**/*.txt')]\n",
    "print(len(paths))\n",
    "paths[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# initialize\n",
    "tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=True,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=False\n",
    ")\n",
    "# and train\n",
    "tokenizer.train(files=paths, vocab_size=50_000, min_frequency=2,\n",
    "                limit_alphabet=1000, wordpieces_prefix='##',\n",
    "                special_tokens=['[PAD', '[UNK]', '[CLS]', '[SEP]', '[MASK]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/bert_sl\\\\-vocab.txt']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save tokenizer\n",
    "import os\n",
    "\n",
    "#os.mkdir('../data/bert_sl')\n",
    "tokenizer.save_model('../data/bert_sl', 'sl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2, 5018, 23901, 16, 2250, 2097, 1954, 16909, 1026, 1948, 36018, 7182, 18, 3], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('../data/bert_sl/sl-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[CLS]', 'pravih', '!', '##red', '##raj', '?', '[SEP]')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data/bert_sl/sl-vocab.txt', 'r', encoding='utf-8') as fp:\n",
    "    vocab = fp.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "dec\n",
      "##ek\n",
      "in\n",
      "deklica\n",
      "ne\n",
      "uz\n",
      "##i\n",
      "##vata\n",
      "ob\n",
      "poslu\n",
      "##san\n",
      "##ju\n",
      "glasbe\n",
      "in\n",
      "prepe\n",
      "##vanju\n",
      "novih\n",
      "melodi\n",
      "##j\n",
      ",\n",
      "pa\n",
      "##c\n",
      "pa\n",
      "raje\n",
      "prisluh\n",
      "##neta\n",
      "dobri\n",
      "knjigi\n",
      "in\n",
      "uz\n",
      "##i\n",
      "##vata\n",
      "ob\n",
      "kozarcu\n",
      "dobrega\n",
      "vina\n",
      "in\n",
      "prijetni\n",
      "dru\n",
      "##zbi\n",
      ".\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "stavek = 'Tukaj lahko uporabnik napiše poljuben stavek v slovenščini.'\n",
    "tokens = tokenizer(stavek)['input_ids']\n",
    "for t in tokens:\n",
    "    print(vocab[t])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26602ab9730260b5ce0f6b9e387bf735a6962b6d9b4f101674e777f3a1c7823f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
