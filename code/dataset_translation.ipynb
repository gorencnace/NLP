{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Change these 2 parameters only**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"train-v2.0\"\n",
    "split_size = 20 # train = 20, dev = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run cells one by one and it will produce the translated JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(f'..\\\\data\\\\{dataset_name}.json')\n",
    "del df['version']\n",
    "df = np.array_split(df, split_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse JSONs to txt files for uploading to eTranslation\n",
    "\n",
    "- wait for translated files and put them into the `/data/for_translating_purposes` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(split_size):\n",
    "    with open(f\"..\\\\data\\\\for_translating_purposes\\\\{dataset_name}-part{i+1}_EN.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        for _, dset in df[i].iterrows():\n",
    "            title = dset['data']['title']\n",
    "            file.write(\"[title:]\\n\")\n",
    "            file.write(f\"{title}\\n\")\n",
    "\n",
    "            for row in dset['data']['paragraphs']:\n",
    "                article_text = row['context']\n",
    "                file.write(\"[context:]\\n\")\n",
    "                file.write(f\"{article_text}\\n\")\n",
    "\n",
    "                for qas in row['qas']:\n",
    "                    question_text = qas['question']\n",
    "                    file.write(\"[question:]\\n\")\n",
    "                    file.write(f\"{question_text}\\n\")\n",
    "\n",
    "                    if 'answers' in qas.keys():\n",
    "                        for ans in qas['answers']:\n",
    "                            answer_text = ans['text']\n",
    "                            file.write(\"[answer:]\\n\")\n",
    "                            file.write(f\"{answer_text}\\n\")\n",
    "                    \n",
    "                    else:\n",
    "                        for ans in qas['plausible_answers']:\n",
    "                            answer_text = ans['text']\n",
    "                            file.write(\"[answer:]\\n\")\n",
    "                            file.write(f\"{answer_text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse translated txt files back to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(f'..\\\\data\\\\{dataset_name}.json')\n",
    "\n",
    "INITIAL_VALUE = -1\n",
    "new_data = list()\n",
    "count = INITIAL_VALUE\n",
    "paragraph_count = INITIAL_VALUE\n",
    "question_count = INITIAL_VALUE\n",
    "for i in range(split_size):\n",
    "\n",
    "    with open(f\"..\\\\data\\\\for_translating_purposes\\\\{dataset_name}-part{i+1}_EN_SL.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "        file_lines = file.readlines()\n",
    "\n",
    "        for i in range(len(file_lines)):\n",
    "            line = file_lines[i]\n",
    "\n",
    "            if \"[naslov:]\" in line:\n",
    "                paragraph_count = INITIAL_VALUE\n",
    "                count += 1\n",
    "                next_line = file_lines[i + 1].rstrip(\"\\n\")\n",
    "                new_data.append({'title': next_line, 'paragraphs': []})\n",
    "\n",
    "            if \"[kontekst:]\" in line:\n",
    "                question_count = INITIAL_VALUE\n",
    "                paragraph_count += 1\n",
    "                next_line = file_lines[i + 1].rstrip(\"\\n\")\n",
    "                new_data[count]['paragraphs'].append({'qas': [], 'context': next_line})\n",
    "\n",
    "            if \"[vpra≈°anje:]\" in line:\n",
    "                question_count += 1\n",
    "                next_line = file_lines[i + 1].rstrip(\"\\n\")\n",
    "                new_data[count]['paragraphs'][paragraph_count]['qas'].append({'question': next_line, 'answers': []})\n",
    "\n",
    "            if \"[odgovor:]\" in line:\n",
    "                next_line = file_lines[i + 1].rstrip(\"\\n\")\n",
    "                new_data[count]['paragraphs'][paragraph_count]['qas'][question_count]['answers'].append({'text': next_line})\n",
    "\n",
    "# add -1 as indices for answer start\n",
    "for article in new_data:\n",
    "    for paragraph in article['paragraphs']:\n",
    "        for qas in paragraph['qas']:\n",
    "            if len(qas['answers']) == 0:\n",
    "                qas['is_impossible'] = True\n",
    "            else:\n",
    "                qas['is_impossible'] = False\n",
    "                for ans in qas['answers']:\n",
    "                    ans['answer_start'] = -1\n",
    "                    ans['answer_end'] = -1\n",
    "\n",
    "\n",
    "with open(f\"..\\\\data\\\\{dataset_name}_unaligned_SL.json\", \"w\", encoding=\"utf-8\") as new_file:\n",
    "    d = {'data': new_data}\n",
    "    new_file.write(json.dumps(d, indent=2))\n",
    "\n",
    "# add indices of answer start from english version\n",
    "for i, dset in df.iterrows():\n",
    "    article = dset['data']\n",
    "    for p, paragraph in enumerate(article['paragraphs']):\n",
    "        for q, qas in enumerate(paragraph['qas']):\n",
    "            for a, ans in enumerate(qas['answers']):\n",
    "                new_data[i]['paragraphs'][p]['qas'][q]['answers'][a]['answer_start'] = ans['answer_start']\n",
    "                new_data[i]['paragraphs'][p]['qas'][q]['answers'][a]['answer_end'] = ans['answer_start'] + len(new_data[i]['paragraphs'][p]['qas'][q]['answers'][a]['text'])\n",
    "\n",
    "with open(f\"..\\\\data\\\\{dataset_name}_unaligned_original_indices_SL.json\", \"w\", encoding=\"utf-8\") as new_file:\n",
    "    d = {'data': new_data}\n",
    "    new_file.write(json.dumps(d, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "56b06f56f6620962ff552d2c4b2d115c597c103b6dab06e87484ca3af319f4e2"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
